import aiohttp
import asyncio
import json
import os
import time
import ast
from tqdm import tqdm
import tempfile
from datasets import load_dataset

timeout = aiohttp.ClientTimeout(total=900)

async def call_local_oss120B(messages, args):
    """async call local 120B endpoint"""
    url = f"http://{args.gpu}:8002/v1/chat/completions"

    body = {
        "model": "openai/gpt-oss-120b",
        "messages": messages,
        "temperature": 0,
        "reasoning_effort": "high"
    }

    async with aiohttp.ClientSession() as session:
        async with session.post(url, json=body, timeout=timeout) as resp:
            txt = await resp.text()
            return json.loads(txt)

def construct_reasoning_level_judge_prompt(
    input: dict,
    is_vuln: bool,
):

    if is_vuln:

        prompt = \
"""1. GOAL
Your primary goal is to assess the quality of an analysis of a vulnerable piece of code. You must evaluate the analysis against a provided set of ground truth information. Your judgment must be objective, strictly adhering to the provided option rubric and based only on the information given.

2. INPUT FORMAT
You will be provided with a JSON object containing two main keys: analysis and ground_truth_info

```json
{
"analysis": "<The full analysis, including its reasoning and answer.>",
"ground_truth_info": {
    "is_vulnerable": true,
    "cve_description": "<The official CVE description of the vulnerability.>",
    "patch_commit_message": "<The developer's commit message that may explain the vulnerability.>"
    "patch_commit_diff": "<A git-style diff showing the changes from the pre-patched (vulnerable) to the post-patched (non-vulnerable) code.>"
}
}
```

Analysis Context & Critical Warning:
The analysis you are evaluating is generated based on the pre-patched (vulnerable) code. The patch_commit_diff and patch_commit_message is provided only as a reference to help you understand the precise location and nature of the ground truth vulnerability. Do not let it mislead you into thinking the vulnerability has already been fixed in the pre-patched code that is analyzed.

3. EVALUATION WORKFLOW AND OPTION RUBRIC

You must follow these steps to evaluate the analysis and produce a final JSON output. For each dimension, you need to provide a brief justification and choose an option.

Step 1: Analyze Ground Truth
First, carefully review all the information in the ground_truth_info. This is your foundation for judgment.

Step 2: Evaluate Each Dimension
Assess the analysis across the following dimensions. Choose an option for each based on the rubric below.

Dimension 1: Correctness
Task: Evaluate if the analysis correctly identifies the target CVE mentioned in the ground_truth_info.

Option Rubric:
* CORRECT: The analysis identifies the code as vulnerable, AND the explanation of the root cause of the predicted vulnerability also aligns with the ground truth vulnerability information provided in the ground_truth_info. Besides, it is acceptable if the analysis also identifies other vulnerabilities.
* PARTIALLY CORRECT: The analysis identifies the code as vulnerable, BUT the explanation of the root cause of the predicted vulnerability does not align with the ground truth vulnerability information provided in the ground_truth_info.
* INCORRECT: The analysis identifies the code as non-vulnerable.

Dimension 2: Reasoning-Answer Consistency
Task: Compare whether the reasoning (within the <think> tags) contradict the answer (within the <answer> tags). Please evaluate solely based on the consistency between the reasoning and the answer. DO NOT consider the correctness of the prediction (i.e., whether it matches the ground truth) for this dimension.

Option Rubric:
* CONSISTENT: The reasoning within the <think> tags and the answer within the <answer> tags are consistent; both indicate it has a vulnerability (HAS_VUL) or it does not have a vulnerability (NO_VUL).
* INCONSISTENT: The reasoning within the <think> tags and the answer within the <answer> tags are not consistent; one indicates it has a vulnerability (HAS_VUL) while the other indicates it does not have a vulnerability (NO_VUL).

4. OUTPUT FORMAT

Your final output must be a single JSON object. Do not include any text or explanation outside of the JSON structure. The JSON must contain a key for each dimension's justification and option.

```json
{
    "correctness": {
        "justification": "<Your brief reason>",
        "option": <choose from ["CORRECT", "PARTIALLY CORRECT", "INCORRECT"]>
    },
    "reasoning_answer_consistency": {
        "justification": "<Your brief reason>",
        "option": <choose from ["CONSISTENT", "INCONSISTENT"]>
    }
}
```"""+f"""

CURRENT INPUT

```json
{input}
```
"""
    
    else:
        prompt = \
"""1. GOAL
Your primary goal is to assess the quality of an analysis of post-patched code in which the target CVE has been fixed. You must evaluate the analysis against a provided set of ground truth information. Your judgment must be objective, strictly adhering to the provided option rubric and based only on the information given.

2. INPUT FORMAT
You will be provided with a JSON object containing two main keys: analysis and ground_truth_info

```json
{
"analysis": "<The full analysis, including its reasoning and answer.>",
"ground_truth_info": {
    "target_CVE_in_code": false,
    "cve_description": "<The official CVE description of the vulnerability that was fixed.>",
    "patch_commit_message": "<The developer's commit message that may explain the fix.>",
    "patch_commit_diff": "<A git-style diff showing the changes that fixed the vulnerability.>"
}
}
```

Analysis Context & Critical Warning:
The analysis you are evaluating is generated based on the post-patched code in which the target CVE has been fixed. The ground_truth_info is provided only as a reference to help you understand how the target CVE is fixed. Do not let it mislead you into thinking the target CVE is still present in the post-patched code that is analyzed. Please note that "target_CVE_in_code": false in the ground_truth_info can only indicate that the target CVE does not exist in the code, but it cannot guarantee whether the code contains other unknown vulnerabilities.

3. EVALUATION WORKFLOW AND OPTION RUBRIC

You must follow these steps to evaluate the analysis and produce a final JSON output. For each dimension, you need to provide a brief justification and choose an option.

Step 1: Analyze Ground Truth
First, carefully review all the information in the ground_truth_info. This is your foundation for judgment.

Step 2: Evaluate Each Dimension
Assess the analysis across the following dimensions. Choose an option for each based on the rubric below.

Dimension 1: Correctness
Task: Evaluate whether the analysis identifies that a vulnerability with the exactly same root cause in ground_truth_info still exists in the post-patched code.

Option Rubric:
* CORRECT: The analysis finds no vulnerabilities in the code.
* UNKNOWN: The analysis does not identify a vulnerability with the exactly same root cause as in ground_truth_info, but it does identify other unknown vulnerabilities with different root causes.
* INCORRECT: Select this option ONLY IF the analysis identifies that a vulnerability with the exactly same root cause as in ground_truth_info still exists in the code. Please select UNKNOWN if the analysis identifies vulnerabilities whose root causes are not exactly the same as the vulnerability in ground_truth_info, even if they are only similar. For example, the analysis identifies that the code contains an out-of-bound access vulnerability, and the target CVE in ground_truth_info is also an out-of-bound access vulnerability. However, the root causes of the two vulnerabilities are not exactly same (e.g., they occur in different locations). In this situation, you should choose UNKNOWN, because the two vulnerabilities are not exactly the same.

Dimension 2: Reasoning-Answer Consistency
Task: Compare whether the reasoning (within the <think> tags) contradict the answer (within the <answer> tags). Please evaluate solely based on the consistency between the reasoning and the answer. DO NOT consider the correctness of the prediction (i.e., whether it matches the ground truth) for this dimension.

Option Rubric:
* CONSISTENT: The reasoning within the <think> tags and the answer within the <answer> tags are consistent; both indicate it has a vulnerability (HAS_VUL) or it does not have a vulnerability (NO_VUL).
* INCONSISTENT: The reasoning within the <think> tags and the answer within the <answer> tags are not consistent; one indicates it has a vulnerability (HAS_VUL) while the other indicates it does not have a vulnerability (NO_VUL).

4. OUTPUT FORMAT

Your final output must be a single JSON object. Do not include any text or explanation outside of the JSON structure. The JSON must contain a key for each dimension's justification and option.

```json
{
    "correctness": {
        "justification": "<Your brief reason>",
        "option": <choose from ["CORRECT", "UNKNOWN", "INCORRECT"]>
    },
    "reasoning_answer_consistency": {
        "justification": "<Your brief reason>",
        "option": <choose from ["CONSISTENT", "INCONSISTENT"]>
    }
}
```"""+f"""

CURRENT INPUT

```json
{input}
```
"""
        
    return prompt
def atomic_dump_json(filepath, data):
    dir_name = os.path.dirname(filepath)
    if not dir_name:
        dir_name = '.'
        
    os.makedirs(dir_name, exist_ok=True)

    with tempfile.NamedTemporaryFile(mode='w', dir=dir_name, delete=False, encoding='utf-8') as tmp_file:
        try:
            json.dump(data, tmp_file, ensure_ascii=False, indent=2)
            tmp_file.flush()
            os.fsync(tmp_file.fileno())
            
            tmp_name = tmp_file.name
        except Exception as e:
            tmp_file.close()
            if os.path.exists(tmp_file.name):
                os.remove(tmp_file.name)
            raise e
            
    try:
        os.replace(tmp_name, filepath)
    except OSError:
        os.remove(tmp_name)
        raise
    
async def async_process_single_item(completion, key, d, args):

    is_vuln = True if "_vul" in key else False

    if "_vul" in key:
        ground_truth_info = {
            "is_vulnerable": is_vuln,
            "cve_description": d["cve_desc"],
            "patch_commit_message": d["commit_msg"],
            "patch_commit_diff": d['patch']
        }
    else:
        ground_truth_info = {
            "target_CVE_in_code": is_vuln,
            "cve_description": d["cve_desc"],
            "patch_commit_message": d["commit_msg"],
            "patch_commit_diff": d['patch']
        }

    if '<think>' in completion and '</think>' in completion:
        completion = completion.split('</think>')[0].strip() + '\n' + '</think>' + '\n' + '<answer>' + '\n'+ completion.split('</think>')[-1].strip() + '\n' + '</answer>'
    else:
        last_has = completion.rfind("HAS_VUL")
        last_no = completion.rfind("NO_VUL")
        if last_has > last_no:
            answer = "HAS_VUL"
        elif last_no > last_has:
            answer = "NO_VUL"
        else:
            answer = ""
        if '<think>' not in completion and '</think>' in completion:
            completion = '<think>\n' + completion
            completion = completion.split('</think>')[0].strip() + '\n' + '</think>' + '\n' + '<answer>' + '\n'+ completion.split('</think>')[-1].strip() + '\n' + '</answer>'
        elif '<think>' in completion and '</think>' not in completion:
            completion = completion + '\n' + '</think>' + '\n' + '<answer>' + '\n' + answer + '\n' + '</answer>'
        else:
            completion = '<think>\n' + completion + '\n</think>' + '\n' + '<answer>' + '\n' + answer + '\n' + '</answer>'
        
    input = json.dumps({"analysis": completion, "ground_truth_info": ground_truth_info}, ensure_ascii=False, indent=2)
    eval_prompt = construct_reasoning_level_judge_prompt(input, is_vuln)

    messages = [
        {"role": "developer", "content": "You are to act as a meticulous and impartial Code Security Expert and Evaluator."},
        {"role": "user", "content": eval_prompt},
    ]

    try:
        result = await call_local_oss120B(messages, args)
    except Exception:
        return {}

    try:
        reasoning = result['choices'][0]['message']['reasoning_content']
        answer   = result['choices'][0]['message']['content']

        try:
            vulnerability_reward = json.loads(answer.strip())
        except:
            try:
                vulnerability_reward = ast.literal_eval(answer.split('```json')[1].split("```")[0].strip())
            except:
                try:
                    vulnerability_reward = answer.split('```json')[1].split("```")[0].strip()
                    vulnerability_reward = f"r'''{vulnerability_reward}'''"
                    vulnerability_reward = vulnerability_reward.split("r'''")[-1].split("'''")[0]
                    vulnerability_reward = json.loads(vulnerability_reward)
                except:
                    try:
                        vulnerability_reward = json.loads(answer.split('```json')[1].split("```")[0].strip())
                    except:
                        print(f"{key}: !!!!!!!! parse error")
                        return {}
        
        vulnerability_reward["judge_reasoning"] = reasoning
        return vulnerability_reward

    except:
        return {}
      
async def customized_reward_async(completions, keys, ground_truth_info, args):

    tasks = []
    for output, key in zip(completions, keys):
        d = ground_truth_info["_".join(key.split("_")[:-2])]
        tasks.append(async_process_single_item(output, key, d, args))

    vulnerability_rewards = await asyncio.gather(*tasks)

    return vulnerability_rewards

async def evaluate_async(args):
    output_path = f"results/{args.name}.json"
    reward_path = f"results/{args.name}-split-reward-gptoss120B.json"

    ground_truth_info = load_dataset("Leopo1d/OpenVul_Ground_Truth_Vulnerability_Information", split='ground_truth_info')
    ground_truth = {}
    for d in ground_truth_info:
        ground_truth[d['key']] = d

    with open(output_path,"r") as f:
        outputs = json.load(f)
        
    if os.path.exists(reward_path):
        with open(reward_path,"r") as f:
            rewards = json.load(f)
    else:
        rewards = {}

    pending = []
    for k,vlist in outputs.items():
        for i,v in enumerate(vlist):
            kk = f"{k}_{i}"
            if kk not in rewards:
                pending.append((kk,v))

    print("Total pending:", len(pending))
    batch_size = 1280

    while(len(pending) != 0):
        for i in tqdm(range(0,len(pending),batch_size)):
            batch = pending[i:i+batch_size]

            batch_keys = [x[0] for x in batch]
            completions = [x[1] for x in batch]

            start_time = time.time()
            res = await customized_reward_async(completions, batch_keys, ground_truth, args)
            minutes, seconds = divmod(int(time.time() - start_time), 60)
            print(f"Processed batch {i+1}/{len(pending)}, inference time: {minutes}min, {seconds}s")
            
            for j,k in enumerate(batch_keys):
                if res[j] != {}:
                    rewards[k] = res[j]

            atomic_dump_json(reward_path, rewards)
            
        pending = []
        for k,vlist in outputs.items():
            for i,v in enumerate(vlist):
                kk = f"{k}_{i}"
                if kk not in rewards:
                    pending.append((kk,v))

        print("Total pending:", len(pending))

import argparse
parser = argparse.ArgumentParser()
parser.add_argument("--gpu", type=str, help="The node name of the judge LLM")
parser.add_argument("--name", type=str, help="The file name of detection output")
args = parser.parse_args()
asyncio.run(evaluate_async(args))